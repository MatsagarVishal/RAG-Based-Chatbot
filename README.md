# ğŸ§  RAG-Based Chatbot for Any Website URL

A  headless Retrieval-Augmented Generation (RAG) backend** that crawls any website, builds a per-site knowledge base, and answers user questions using semantic search + LLMs.

This project is designed to be:
- âœ… Multi-user
- âœ… Deployable
- âœ… Frontend-agnostic (React, Streamlit, mobile, etc.)
- âœ… Scalable & modular

---

## ğŸš€ Features

- ğŸŒ Crawl any public website (up to configurable depth)
- ğŸ§¹ Clean & preprocess HTML content
- âœ‚ï¸ Chunk text intelligently
- ğŸ§  Generate embeddings using Sentence Transformers
- ğŸ“¦ Store vectors using FAISS (per-website isolation)
- ğŸ” Retrieve relevant context via semantic search
- ğŸ¤– Generate answers using **Groq LLMs**
- ğŸ§© Headless REST APIs (FastAPI)

---

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend   â”‚  (React / Streamlit / Any Client)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTPS (JSON)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     FastAPI Backend      â”‚
â”‚  (Stateless, Headless)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Crawl Service   â”‚
 â”‚  (Website â†’ KB)  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Knowledge Base (KB)  â”‚
 â”‚  - Clean text        â”‚
 â”‚  - Chunking          â”‚
 â”‚  - Embeddings        â”‚
 â”‚  - FAISS Index       â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   RAG Chat Service   â”‚
 â”‚  - Semantic Search  â”‚
 â”‚  - Prompt Builder   â”‚
 â”‚  - Groq LLM         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Detailed Component Architecture

```
rag-backend/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py              # FastAPI app entry
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ crawl.py         # /api/crawl
â”‚       â””â”€â”€ chat.py          # /api/chat
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ crawl_service.py     # Crawl + KB workflow
â”‚   â””â”€â”€ chat_service.py      # RAG query workflow
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”œâ”€â”€ crawler.py       # Website crawling logic
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ kb/
â”‚   â”‚   â”œâ”€â”€ build_kb.py      # Chunking + embedding
â”‚   â”‚   â””â”€â”€ vector_store.py  # FAISS save/load
â”‚   â”‚
â”‚   â””â”€â”€ rag/
â”‚       â””â”€â”€ qa_chain.py      # Retrieval + LLM logic
â”‚
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ crawl.py             # Request/response models
â”‚   â””â”€â”€ chat.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ url_hash.py          # URL â†’ kb_id
â”‚
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ <kb_id>/         # One folder per website
â”‚           â”œâ”€â”€ raw_pages.json
â”‚           â”œâ”€â”€ faiss.index
â”‚           â””â”€â”€ metadata.pkl
â”‚
â”œâ”€â”€ .env                     # Environment variables
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ End-to-End Data Flow

### 1ï¸âƒ£ Build Knowledge Base

```
Client â†’ POST /api/crawl
        â†“
Backend:
- Validate URL
- Generate kb_id
- Crawl website
- Clean & extract text
- Chunk content
- Generate embeddings
- Store FAISS index
```

### 2ï¸âƒ£ Ask a Question

```
Client â†’ POST /api/chat
        â†“
Backend:
- Load FAISS index by kb_id
- Embed user question
- Retrieve top-k chunks
- Build RAG prompt
- Query Groq LLM
- Return answer + sources
```

---

## ğŸŒ API Reference

### ğŸ”¹ Health Check
```
GET /health
```
Response:
```json
{ "status": "ok" }
```

---

### ğŸ”¹ Crawl Website
```
POST /api/crawl
```
Request:
```json
{
  "url": "https://example.com"
}
```
Response:
```json
{
  "kb_id": "example_com",
  "status": "completed",
  "pages_crawled": 1
}
```

---

### ğŸ”¹ Chat with Website
```
POST /api/chat
```
Request:
```json
{
  "kb_id": "example_com",
  "question": "What is this website about?"
}
```
Response:
```json
{
  "answer": "Example.com is used for illustrative purposes.",
  "sources": ["https://example.com"]
}
```

---

## ğŸ” Environment Variables

Create a `.env` file:

```env
GROQ_API_KEY=your_groq_api_key_here
```

---

## ğŸ§  Design Decisions & Rationale

- **Stateless backend** â†’ scalable & cloud-friendly
- **Per-website vector stores** â†’ multi-user safe
- **FAISS** â†’ fast local vector search
- **Sentence Transformers** â†’ high-quality embeddings
- **Groq LLMs** â†’ fast inference & free tier
- **FastAPI** â†’ async, high performance, OpenAPI support

---

## ğŸš€ Deployment Ready

This backend is ready for:
- Dockerization
- Render / Railway / AWS / Azure
- Integration with React or any frontend

---

## ğŸ† What This Project Demonstrates

- Real-world RAG architecture
- Backend system design
- API-first development
- LLM integration
- Vector databases
- Clean, modular codebase

---

## ğŸ“Œ Future Enhancements

- Authentication & rate limiting
- Chat history & sessions
- Streaming responses
- Hybrid search (BM25 + vectors)
- UI dashboard (React)

---

## ğŸ³ Docker Deployment

1.  **Environment Variables**: Ensure your `.env` file exists and contains `GROQ_API_KEY`. The `docker-compose.yml` is configured to read this file automatically.

2.  **Build and Run**:
    ```bash
    docker-compose up --build -d
    ```

3.  **Persistence**: The `storage/` directory is mounted to the container. Data crawled inside Docker will appear in your local `storage` folder and persist across restarts.

4.  **Access**: The API will be available at `http://localhost:8000`.

---
